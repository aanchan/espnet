# This configuration reuqires 1 gpus in the case of each gpu memory = 12GB, and it takes 3~4 days.
# If you want to accelerate the training with multi-gpus, please use train_pytorch_transformer.v1.yaml config.
# Or you can manually increase batch-bins and decrease accum-grad.
# Note that transoformer requires a large batchsize > 64 in average according to the original paper.

# 2020.04 Wen-Chin Huang
# uses lamb
# I use this for decoder pretraining with ASR encoder, so some parts are changed to align with ASR encoder

# network architecture related
model-module: espnet.nets.pytorch_backend.e2e_vc_transformer:Transformer
eprenet-conv-layers: 0  # one more linear layer w/o non-linear will be added for 0-centor
eprenet-conv-filts: 0
eprenet-conv-chans: 0
dprenet-layers: 2  # one more linear layer w/o non-linear will be added for 0-centor
dprenet-units: 256
adim: 384
aheads: 4
elayers: 6
eunits: 1536
dlayers: 6
dunits: 1536
postnet-layers: 5
postnet-filts: 5
postnet-chans: 256
use-masking: True
bce-pos-weight: 5.0
use-batch-norm: True
use-scaled-pos-enc: True
encoder-normalize-before: True    
decoder-normalize-before: False
encoder-concat-after: False
decoder-concat-after: False
reduction-factor: 2
transformer-input-layer: conv2d-scaled-pos-enc
use-speaker-embedding: true
spk-embed-integration-type: add

# minibatch related
batch-sort-key: input # shuffle or input or output
batch-bins: 6486000   # 64 * (870 * 80 + 180 * 80)
                      # batch-size * (max_out * dim_out + max_in * dim_in) 

# training related
transformer-init: pytorch
transformer-warmup-steps: 4000
transformer-lr: 0.1
initial-encoder-alpha: 1.0
initial-decoder-alpha: 1.0
eprenet-dropout-rate: 0.0
dprenet-dropout-rate: 0.5
postnet-dropout-rate: 0.5
transformer-enc-dropout-rate: 0.1
transformer-enc-positional-dropout-rate: 0.1
transformer-enc-attn-dropout-rate: 0.1
transformer-dec-dropout-rate: 0.1
transformer-dec-positional-dropout-rate: 0.1
transformer-dec-attn-dropout-rate: 0.1
transformer-enc-dec-attn-dropout-rate: 0.1
use-guided-attn-loss: true
num-heads-applied-guided-attn: 2
num-layers-applied-guided-attn: 2
modules-applied-guided-attn: ["encoder-decoder"]
guided-attn-loss-lambda: 10
dec-init: "downloads/phn_train_no_dev_pytorch_train_pytorch_transformer+spkemb/results/model.last1.avg.best"
dec-init-mods: decoder,postnet,feat_out,prob_out
freeze-mods: decoder,postnet,feat_out,prob_out
enc-init-mods: encoder
# optimization related
opt: lamb
lr: 0.001
accum-grad: 2
grad-clip: 1.0
weight-decay: 0.0
patience: 0
epochs: 1000  # 1,000 epochs * 969 batches / 6 accum-grad = 161,500 iters
              # 1000 epochs * (80000/64) / 1 accum-grad = 1250000 iters

# other
save-interval-epoch: 10
